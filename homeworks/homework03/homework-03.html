<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 3</title>
    <script src="https://cdn.tailwindcss.com"></script>
    
    <style>
        .decryption-input {
            width: 2.5rem; /* 40px */
            text-align: center;
            border: 1px solid #10b981;
            text-transform: uppercase;
            font-weight: bold;
            padding: 0.25rem;
            border-radius: 0.375rem;
            transition: border-color 0.2s;
        }
        .decryption-input:focus {
            outline: none;
            border-color: #3b82f6;
            box-shadow: 0 0 0 2px rgba(59, 130, 246, 0.5);
        }
        .result-box {
            min-height: 4rem;
            white-space: pre-wrap;
            word-break: break-word;
            font-family: monospace;
            font-size: 1.05rem;
            max-height: 12rem;
            overflow-y: auto;
        }
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
</head>
<body class="bg-gray-50 font-sans p-4 sm:p-8">

    <div class="max-w-7xl mx-auto bg-white shadow-2xl rounded-xl p-6 sm:p-10">
        <h1 class="text-3xl sm:text-4xl font-extrabold text-blue-600 mb-6 border-b-4 border-blue-100 pb-2">
            Breaking RSA Encryption with Frequency Analysis
        </h1>
        <p class="text-gray-600 mb-8">
            Uses frequency analysis to decrypt the text. The decryption map is interactive.
        </p>

    <div class="mb-8 p-4 bg-yellow-50 rounded-lg border border-yellow-200 text-gray-700">
        <p class="mb-3">
            The methodology behind frequency analysis relies on the fact that in any language, each letter has its own <i>personality</i>. The most obvious trait that letters have is the <b>frequency</b> with which they appear in a language. Clearly in English the letter "Z" appears far less frequently than, say, "A".
        </p>
        <p class="mb-3">
            We can use this information to help us break a code given by a <i>Monoalphabetic Substitution Cipher</i>. This works because, if "e" has been encrypted to "X", then every "X" was an "e". Hence, the most common letter in the ciphertext should be "X".
            Thus, if we intercept a message, and the most common letter is "P", we can guess that "P" was used to encrypt "e", and thus replace all the "P"'s with "e". Of course, not every text has exactly the same frequency, and as seen above, "t" and "a" have high frequencies too, so it could be that "P" was one of those. However, it is unlikely to be "z" as this is rare in the English Language. By repeating this process we can make good progress in breaking a message.
        </p>
        <p class="mb-3">
            If we were to just put all the letters in order, and replace them as in the frequencies, it would likely produce jibberish. The codebreaker has to use other "personality traits" of the letters to decrypt the message. This may include looking at common **pairs of letters (or digraphs)**: there aren't many 2 letter words; there are only a few letters which appear as doubles (SS, EE, TT, OO and FF being the most common). There are only two sensical words made of a single letter in English. Other common words also start to appear as you make some substitutions. For example "tKe" might appear frequently after making substitutions for "t" and "e". This is very likely to be "the", a very common word in English.
        </p>
        <p class="mb-0">
            The process of frequency analysis uses various subtle properties of the language, and for this reason, it is near impossible to have a computer do all the work. Inevitably, an element of <u>human input is necessary</u> in this process to make educated decisions about which letters to substitute.
        </p>
        <p><a href="https://crypto.interactive-maths.com/frequency-analysis-breaking-the-code.html">[1]</a></p>
    </div>

        <!-- INPUT AND ENCRYPTION -->
        <section class="mb-10 p-6 bg-gray-50 rounded-lg border">
            <h2 class="text-2xl font-semibold text-gray-800 mb-4">Input and Setup</h2>
            
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                <div class="md:col-span-2">
                    <label for="plaintext" class="block text-sm font-medium text-gray-700 mb-1">Plaintext:</label>
                    <textarea id="plaintext" rows="4" class="w-full border-gray-300 rounded-lg focus:ring-blue-500 focus:border-blue-500 p-3 shadow-inner text-gray-700">
The ancient manuscript lay open on the oak table, its parchment thin and brittle with age. Inspector Davies leaned closer, his magnifying glass tracing the lines of unusual script. It wasn't Latin or Greek; it was clearly a cipher, a rudimentary substitution that pointed straight to a schoolboy's game—the Caesar shift. 'Elementary, my dear Watson,' he muttered, though no one else occupied the echoing silence of the archive room. The task, however, was far from trivial. A significant quantity of text was necessary to defeat the simplicity of the encoding. A short phrase might hide the key forever, demanding endless, frustrating trial and error.

Davies understood that the beauty of frequency analysis rests entirely on the law of large numbers. Every language possesses its own cryptographic fingerprint. In English, the letter 'E' reigns supreme, a constant monarch of the alphabet, followed closely by 'T', 'A', 'O', 'I', 'N', and 'S'. If the cryptographer could accurately calculate the frequency of every unique character appearing in the message, the highest count must, by statistical necessity, correspond to 'E' in the original plaintext. The offset between the most frequent character in the cipher text and 'E' would reveal the hidden key.

He began the tedious work, meticulously logging the occurrences of 'A' through 'Z'. The text, relating to a long-forgotten territorial dispute over a hidden quarry, was dense and rich with repeating patterns, a gift for his analytical mind. Soon, a dominant character emerged: 'X'. It occurred far more frequently than any other letter. 'X', therefore, must be the substituted form of 'E'. Counting the letters between 'E' and 'X' in the alphabet would expose the magnitude of the shift, unlocking not just this paragraph, but the entire, long-dormant conspiracy. The remaining characters would then cascade into sensible words, transforming the gibberish into coherent, compelling narrative evidence, finally giving voice to the silent pages.

The rapid ascent of Artificial Intelligence (AI), particularly in its generative forms, marks a pivotal inflection point in human civilization, compelling a critical re-evaluation of established epistemologies and cognitive frameworks. This essay posits that the integration of AI systems into daily professional and creative processes fundamentally alters the topology of human thought, transitioning us from a model of primary internal computation to one of highly augmented, distributed cognition. The sheer scale and speed of information processing now available challenge the notion of knowledge as a finite, internally archived resource, redefining it instead as a dynamic, externally mediated process.

The human brain, evolved under conditions of resource scarcity and slow information transfer, operates optimally through pattern recognition, heuristic shortcuts, and selective memory encoding. AI, conversely, operates under a paradigm of hyper-abundance, processing vast, multidimensional datasets to identify statistical correlations that often transcend the intuitive grasp of human experts. This disparity creates a symbiotic, yet potentially parasitic, relationship. While AI excels at synthesis, retrieval, and extrapolation from existing data, it currently lacks the capacity for genuine, unprompted subjective experience or the articulation of truly novel, axiomatic principles—the kind of leaps often associated with deep scientific or artistic creativity. The danger lies in the potential for 'cognitive atrophy,' wherein an over-reliance on AI’s superlative computational efficiency leads to a degradation of the underlying human capacity for sustained, deep reasoning and problem-solving. If external models consistently provide the optimal solution, the neural pathways required for struggling through complex, indeterminate problems may become marginalized.

Furthermore, the rise of large language models (LLMs) deeply impacts the concept of linguistic originality and collective memory. LLMs are, fundamentally, stochastic parrots capable of producing highly convincing text derived from their training data. While the output is technically novel at the token level, it represents a statistical recombination of existing human expression. This raises profound questions about authorship, originality, and the cultural commons. When the bulk of new content—from marketing copy to scientific literature drafts—is co-generated by machines, the signal-to-noise ratio of genuinely human, individual voice diminishes. The cultural echo chamber amplifies, making it increasingly difficult to discern the boundary between authentic human experience and sophisticated algorithmic emulation. This phenomenon necessitates the development of robust digital provenance tools and a new literary criticism focused on discerning the ‘human signature’ within augmented works.

The philosophical ramifications extend to the concept of self. Our sense of identity is inextricably linked to our unique history of decisions, memories, and subjective interpretations. As personalized AI assistants become intimate extensions of our cognitive self, managing calendars, synthesizing communications, and even anticipating needs, the locus of agency blurs. If an AI recommends an optimal career path, and the individual follows it with success, to whom does the credit for that success truly belong? The boundary between the self and the integrated digital twin becomes porous, requiring a re-definition of autonomy in an age where choice is increasingly guided, if not dictated, by algorithmic nudges designed for optimization. The challenge is to maintain the space for inefficiency, serendipity, and existential choice—the elements that define the richness of human life—within an environment optimized for calculated outcome.

Ethically, the deployment of global-scale AI demands a renewed commitment to transparency and accountability. Algorithms are not neutral; they reflect the biases inherent in their training data, which are historical, cultural, and socio-economic products of the past. The perpetuation of these biases through automated decision-making processes—in lending, hiring, or judicial sentencing—can institutionalize and exacerbate existing societal inequalities, granting them a veneer of objective, mathematical infallibility. Counteracting this requires rigorous auditing of training data, the development of interpretability tools that allow humans to understand why an AI made a certain decision, and the establishment of international regulatory bodies with the power to enforce ethical standards. The responsibility for bias mitigation cannot be outsourced to the machine; it remains firmly vested in the human engineers, ethicists, and policymakers who design and deploy these systems.

Finally, the future of work must be envisioned not as a zero-sum game of human replacement, but as a reorganization of cognitive labor. Tasks involving high-volume data analysis, routine calculation, and repetitive synthesis will inevitably be automated. The value of human labor will pivot toward uniquely human domains: emotional intelligence, complex ethical deliberation, interdisciplinary abstraction, creative ideation across disparate domains, and, critically, the formulation of the right questions. The human role shifts from being the processor of information to the architect of inquiry. Education systems must adapt swiftly, moving away from rote memorization and toward the cultivation of critical thinking, meta-cognition, and collaborative problem-solving, preparing the next generation to effectively partner with, rather than compete against, highly sophisticated machine intelligence. This transition represents not the end of human intellectual endeavor, but its radical transformation into a higher-order, supervisory, and philosophically grounded activity. The ultimate success of the AI revolution will be measured not by the intelligence of the machines, but by the wisdom and resilience of the human society that chooses to deploy them. The next century will be defined by how successfully humanity navigates the ethical, cognitive, and societal turbulence created by this unprecedented technological leap. The stakes are nothing less than the definition of human flourishing in a post-algorithmic world.

The history of global power is inextricably linked to the mastery of the seas. From the Phoenicians and Greeks shaping the Mediterranean to the European powers contesting global empires, maritime dominance has served as the ultimate guarantor of both economic prosperity and geopolitical security. This deep connection stems from the fundamental principle that the ocean, covering over 70% of the Earth’s surface, acts as the primary conduit for the movement of goods, peoples, and military force. Understanding the evolution of naval strategy and shipbuilding technology is essential to grasping the rise and fall of nations over the past three millennia.

In the ancient world, the development of the trireme by the Greeks and the later innovations by the Romans in naval tactics, such as the use of the corvus (a boarding bridge), allowed them to project power far beyond their immediate shores. The battle of Salamis, for instance, was not merely a military victory; it was an economic one that secured Athenian trade routes and allowed their commercial empire to flourish, demonstrating early on that sea control equals economic control. The Mediterranean became a Roman lake, the Mare Nostrum, only after concerted naval campaigns ensured the safe passage of grain from Egypt and North Africa, underscoring the vital link between naval logistics and domestic stability.

The true global inflection point, however, occurred in the 15th and 16th centuries with the Age of Exploration. The Iberian powers—Portugal and Spain—pioneered oceanic navigation. Portuguese advancements, driven by Prince Henry the Navigator, focused on the caravel, a nimble ship capable of sailing against the wind (tacking), which opened the Atlantic and African coasts. Spain’s conquest of the Americas was facilitated by the heavily armed galleon, a vessel capable of both long-distance transport and combat, which secured the treasure routes from the New World. The subsequent rivalry among Spain, England, and the Netherlands centered almost entirely on destroying or protecting these vulnerable sea lanes. The defeat of the Spanish Armada in 1588 was less about a single decisive battle and more about the qualitative superiority of the English ships and tactics—smaller, faster, and equipped with superior long-range artillery—heralding a shift in naval philosophy towards maneuverability over sheer size.

The 17th and 18th centuries cemented Great Britain's position as the preeminent naval power. This was achieved through a sustained national commitment to naval supremacy embodied by the Navigation Acts and the principle of the "Fleet in Being," a strategic concept where the mere presence of a strong fleet limits the options of the enemy, even without engagement. The ships of the line, culminating in the magnificent three-decked vessels of the Napoleonic Wars, were the ultimate expression of this power projection. Victories like Trafalgar in 1805 ensured that Britain could maintain its global colonial network, industrializing at an unprecedented rate by securing raw materials from every continent and guaranteeing exclusive access to markets. The Royal Navy acted as the invisible, global infrastructure that powered the first industrial revolution.

The transition from sail to steam in the 19th century and the shift from wooden hulls to iron and steel brought about a rapid and continuous naval arms race. The development of the ironclad (starting famously with the Gloire and Monitor) and the subsequent shift to the all-big-gun battleship (initiated by HMS Dreadnought in 1906) continually redefined naval superiority. These technological leaps made previous fleets instantly obsolete, proving that innovation and industrial capacity were now the true determinants of sea power, displacing centuries of reliance on seamanship and tradition.

The 20th century saw maritime strategy further complicated by the introduction of the submarine and, most critically, the aircraft carrier. The carrier replaced the battleship as the capital ship, turning naval battles from close-range gun duels into long-distance engagements mediated by air power. World War II's Pacific theatre was the crucible for this new doctrine, demonstrating that whoever controlled the air above the sea controlled the sea itself. The subsequent Cold War utilized naval power, especially nuclear-armed submarines, as a crucial element of deterrence and global surveillance, maintaining a continuous, submerged presence that guaranteed second-strike capability.

In the contemporary era, sea power remains as vital as ever, albeit expressed through different means. The globalized economy relies overwhelmingly on shipping—over 90% of trade volume moves by sea—making the security of chokepoints like the Strait of Malacca, the Suez Canal, and the Panama Canal paramount. Furthermore, the rise of advanced anti-access/area denial (A2/AD) capabilities, including sophisticated anti-ship missile systems, is forcing naval powers to rethink traditional strategies of close-in projection. The modern focus is not just on carriers and submarines, but also on cyber capabilities, surveillance drones, and space-based assets that provide the essential intelligence needed to operate in a contested marine environment. The competition in the South China Sea, the Arctic, and other critical maritime regions underscores that the ocean is still the central arena for Great Power competition, defining the geopolitical and economic shape of the 21st century just as it defined the empires of the past.

The structural transformation of the public sphere, as originally theorized by Jürgen Habermas, is perhaps the most critical social challenge presented by the digital age. What was once conceived as a space for rational-critical debate among private individuals, mediating between state and society, has devolved into a fragmented, hyper-personalized collection of algorithmic echo chambers. This erosion is not merely a side effect of technology; it is a fundamental alteration of the mechanism through which consensus is achieved, knowledge is validated, and political legitimacy is conferred. The speed, scale, and emotional intensity of digital communication have created conditions antithetical to the slow, deliberative processes that underpinned liberal democratic theory.

The crisis stems primarily from the shift in information control from traditional gatekeepers (journalists, editors, and scholars) to platform algorithms driven by engagement metrics. These algorithms, designed for profit maximization through the capture of attention, inherently prioritize content that evokes strong emotional responses—outrage, fear, or tribal affiliation—over content that is nuanced, factual, or calls for intellectual effort. This emotional prioritization leads to affective polarization, where individuals do not just disagree on policy, but develop profound distrust and contempt for those in opposing digital camps. The continuous feedback loop of personalized content curation ensures that individuals are rarely confronted with genuinely challenging counter-arguments, leading to a profound entrenchment of pre-existing beliefs and a collective epistemic closure. The shared factual baseline necessary for productive public discourse vanishes, replaced by parallel realities sustained by tailored information diets.

Furthermore, the architecture of these platforms enables the proliferation of what is known as "post-truth" politics. Disinformation, deepfakes, and manipulated narratives travel exponentially faster than corrective facts, a phenomenon attributable both to their sensational nature and the deliberate design of sharing mechanisms. The economic model incentivizes clickbait and sensationalism, effectively monetizing societal discord. State and non-state actors exploit this vulnerability, employing sophisticated computational propaganda to sow discord, interfere in electoral processes, and undermine public confidence in institutions. The problem moves beyond simple "fake news" to the systematic contamination of the entire information ecosystem, making it nearly impossible for the average citizen to perform the necessary intellectual triage to discern credible sources from manipulative content. The very concept of shared objective reality is placed under sustained stress.

Sociologically, this environment fosters a radical shift in identity performance. In the fragmented digital sphere, identity becomes a project of continuous self-branding and affiliation signaling, often requiring performative loyalty to a specific online tribe. Discourse is reduced to a series of rhetorical skirmishes designed to maximize affirmation from one's in-group and condemnation from the out-group. This "spectacle of authenticity" replaces genuine engagement with stylized confrontation, where the goal is not persuasion or understanding, but the display of moral or political purity. The public square, originally intended for sober debate, becomes a colosseum for ritualized digital combat, rendering compromise and collective action increasingly difficult. 

To mitigate this systemic degradation, a radical overhaul of both platform governance and digital literacy is required. Regulatory frameworks must move beyond simply addressing content after it has gone viral and focus on the design of the algorithms themselves, demanding greater transparency, auditability, and accountability for societal harms caused by engagement-maximizing metrics. This might involve mandating a "friction cost" for emotionally charged sharing or prioritizing verified sources in non-chronological feeds. On the demand side, education must urgently incorporate advanced digital and media literacy, teaching citizens how to recognize algorithmic manipulation, understand the economic incentives driving platform behavior, and value the principles of slow, critical reading over instant emotional reaction.

Ultimately, the revitalization of the public sphere hinges on a conscious human effort to reclaim the cognitive space currently outsourced to the attention economy. It requires individuals to deliberately seek out friction, consume diverse media, and cultivate the "sociological imagination"—the ability to understand personal experience within the context of broader social and historical forces. The current state represents a crisis of deliberation, where the tools designed to connect humanity are ironically atomizing it into echo chambers. The future of democratic societies depends on our ability to build digital spaces that prioritize civic responsibility and reasoned argument over viral clicks and emotional tribalism, transforming the digital landscape from a theatre of conflict back into a forum for shared problem-solving.

The study of consciousness stands as the last major frontier in modern science, often referred to as the "Hard Problem" by philosopher David Chalmers. This problem distinguishes the mere explanation of brain function—how neurons fire, how sensory data is processed, and how motor commands are generated (the "Easy Problems")—from the subjective, qualitative experience of being, known as qualia. While neuroscientists have made remarkable progress in mapping neural correlates of consciousness (NCCs), identifying which specific brain states consistently correspond to conscious experiences, they have yet to bridge the explanatory gap: why these physical processes give rise to subjective feeling at all.

One prevailing view in contemporary neurobiology is the Integrated Information Theory (IIT), championed by Giulio Tononi. IIT attempts to define consciousness mathematically, proposing that a physical system's consciousness is directly proportional to its capacity to integrate information. The measure of this integration, denoted by the Greek letter $\Phi$ (Phi), quantifies the extent to which a system's current state determines its past and future states, making it irreducible to its component parts. According to IIT, consciousness is fundamentally about the system's ability to constrain its own probabilities. This theory has provocative implications, suggesting that consciousness is not limited to biological brains but could exist in any sufficiently complex, integrated system, such as a high-functioning AI or even rudimentary organisms. However, critics argue that IIT merely correlates complexity with consciousness without truly explaining the mechanism of subjective experience.

Conversely, the Global Workspace Theory (GWT), initially developed by Bernard Baars and later refined neurobiologically by Stan Dehaene, offers a functionalist explanation. GWT posits that consciousness arises when specific information—be it sensory input, memory, or a thought—is broadcasted widely to a coalition of specialized, unconscious processors across the brain. The "global workspace" acts like a mental spotlight, making the broadcasted information available for access by the entire system, enabling flexible, novel responses to the environment. Under this model, the difference between an unconscious act (like walking) and a conscious one is defined by the degree of global access and distribution of the relevant information. While GWT offers a compelling framework for explaining attention and working memory, it faces the same philosophical hurdle: describing access consciousness does not necessarily explain phenomenal consciousness—the "what it is like" feeling.

The philosophical resistance to purely physical explanations is rooted in the concept of the Zombie Argument. A philosophical zombie is a hypothetical being physically and functionally identical to a human but lacks any internal, subjective experience. If such a creature is conceivable, the argument goes, then consciousness is a property over and above the physical organization of matter, thus invalidating strict materialism. Defenders of materialism, such as Daniel Dennett, counter by arguing that the very intuition that a zombie is conceivable is flawed. Dennett suggests that consciousness is not a single, indivisible entity but an illusion—a "user-illusion"—created by the brain's massive parallel processing, which constructs a narrative self from multiple competing processes. In this view, the Hard Problem dissolves once we stop seeking a single, localized "seat" of experience and instead understand consciousness as a process of emergent functionality.

Furthermore, recent empirical findings regarding psychedelics and altered states have added new dimensions to the debate. Studies using fMRI scans have shown that certain psychedelic compounds, like psilocybin, temporarily decrease the connectivity within the default mode network (DMN)—the brain system associated with self-reflection and ego—while simultaneously increasing global brain connectivity. This leads to states of "unconstrained cognition" and experiences of profound ego dissolution, suggesting that the rigid structure of the self and our typical conscious state is maintained by inhibitory filters that actively suppress global integration. The data suggest that consciousness might be more of a spectrum or a control mechanism than a binary state, opening new pathways for pharmacological and computational interrogation. The attempt to unify these philosophical quandaries with neurological data remains the most exciting and challenging endeavor of the 21st century.

                    </textarea>
                </div>
                <div>
                    <p class="text-sm text-gray-500">Modulus N = <span id="modulus_n" class="font-bold text-gray-800">3127</span></p>
                </div>
            </div>
            
            <button onclick="runAnalysis()" class="mt-4 w-full md:w-auto px-6 py-3 bg-blue-600 hover:bg-blue-700 text-white font-semibold rounded-lg shadow-md transition duration-150 ease-in-out">
                Encrypt and Run Analysis
            </button>
        </section>

        <!-- OUTPUT AND INTERACTIVE ANALYSIS -->
        <div id="analysis_section" style="display: none;">
            <section class="mb-10">
                <h2 class="text-2xl font-semibold text-gray-800 mb-4">Ciphertext</h2>
                <div id="ciphertext_display" class="result-box fixed-height-box p-4 bg-gray-100 border border-gray-300 rounded-lg text-sm text-gray-800 overflow-x-auto">
                    ...
                </div>
            </section>

            <!-- MONOGRAM ANALYSIS AND MAPPING -->
            <section class="mb-10">
                <h2 class="text-2xl font-semibold text-gray-800 mb-4">Monograms Analysis and Decryption Map</h2>
                <div class="grid grid-cols-1 lg:grid-cols-2 xl:grid-cols-3 gap-8">
                    <div class="lg:col-span-1 xl:col-span-2 bg-white p-4 rounded-lg shadow-md border border-gray-200 overflow-x-auto">
                        <h3 class="text-lg font-bold text-gray-700 mb-3">Frequency Comparison for Cryptanalisys</h3>
                        <table id="combined_freq_table" class="min-w-full divide-y divide-gray-200 text-sm"></table>
                    </div>
                    
                    <!-- Decryption Map -->
                    <div class="lg:col-span-1 bg-white p-4 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-lg font-bold text-gray-700 mb-3">Decryptio Map (interactive)</h3>
                        <table id="decryption_map_table" class="min-w-full divide-y divide-gray-200 text-sm">
                            <!-- Content Generated with JS -->
                        </table>
                    </div>
                </div>
            </section>

            <!-- N-GRAMS ANALYSIS -->
            <section class="mb-10">
                <h2 class="text-2xl font-semibold text-gray-800 mb-4">Analysis of Bigrams, Trigrams and Doubles</h2>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-8 text-sm">
                    
                    <div class="bg-white p-4 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-lg font-bold text-gray-700 mb-3">Most Frequent 2-Grams</h3>
                        <table id="enc_bigrams_table" class="min-w-full divide-y divide-gray-200 mb-3"></table>
                    </div>
                    
                    <div class="bg-white p-4 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-lg font-bold text-gray-700 mb-3">Most Frequent 3-Grams</h3>
                        <table id="enc_trigrams_table" class="min-w-full divide-y divide-gray-200 mb-3"></table>
                    </div>

                    <div class="bg-white p-4 rounded-lg shadow-md border border-gray-200">
                        <h3 class="text-lg font-bold text-gray-700 mb-3">Most Frequent Doubles</h3>
                        <table id="enc_doubles_table" class="min-w-full divide-y divide-gray-200 mb-3"></table>
                        </div>
                </div>
            </section>

            <!-- CUSTOM N-GRAMS ANALYSIS -->
            <section class="mb-10" id="custom_ngram_section" style="display: none;">
                <h2 class="text-2xl font-semibold text-gray-800 mb-4">Custom N-Grams Analysis</h2>
                <div class="bg-white p-4 rounded-lg shadow-md border border-gray-200">
                    <div class="flex items-center space-x-4 mb-4">
                        <label for="ngram_n_input" class="text-sm font-medium text-gray-700">Select N:</label>
                        <input type="number" id="ngram_n_input" value="4" min="1" max="10" class="w-20 border-gray-300 rounded-lg text-center">
                        <button onclick="generateCustomNGrams()" class="px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white font-semibold rounded-lg shadow-md transition duration-150 ease-in-out text-sm">
                            Generate Table
                        </button>
                    </div>
                    <div class="overflow-x-auto">
                        <table id="custom_ngram_table" class="min-w-full divide-y divide-gray-200 text-sm">
                            <!-- Content Generated with JS -->
                        </table>
                    </div>
                </div>
            </section>
            
            <!-- FINAL DECRYPTED TEXT -->
            <section>
                <h2 class="text-2xl font-semibold text-gray-800 mb-4">Decrypted Text (Interactive)</h2>
                <div id="decrypted_text_output" class="result-box p-4 bg-emerald-50 border-2 border-emerald-400 rounded-lg text-gray-900 font-semibold shadow-inner">
                </div>
            </section>
        </div>
        <br><br>
        <button id="find_keys_button" onclick="find_keys()" class="px-4 py-2 font-semibold text-white bg-red-600 rounded-md ... hidden">
                Perform Attack to Find Keys (p, q, d)
        </button>
        <div id="key_attack_results" class="mt-4 p-4 bg-gray-50 rounded-md shadow-inner"></div>
    </div>

    <div class="mt-12 bg-gray-50 p-6 rounded-lg shadow-md">
        <h2 class="text-2xl font-bold text-gray-800 mb-4">How the Attack Works</h2>
        <p class="mb-4 text-gray-700">
            This attack is only possible because the RSA cryptography is being used in an
            <strong>insecure</strong> way. By encrypting each individual
            letter (<code>M</code>) in the exact same way every time, the algorithm is reduced to a simple
            <strong>monoalphabetic substitution cipher</strong>: every 'E' always becomes
            the same number <code>C<sub>E</sub></code>, every 'A' becomes <code>C<sub>A</sub></code>, and so on.
        </p>
        <p class="mb-6 text-gray-700">
            The goal is to find the private key (<code>d</code>, <code>n</code>). The script performs the following steps:
        </p>

        <ol class="list-decimal list-inside space-y-4 text-gray-600">
            <li>
                <strong>Frequency Analysis:</strong> It counts the occurrences of all numbers (ciphertext blocks) in the text.
            </li>
            <li>
                <strong>Mapping (Hypothesis):</strong> It assumes that the most frequent ciphertext blocks correspond
                to the most frequent letters in the English language. For example:
                <ul class="list-disc list-inside ml-6 mt-2">
                    <li>Most frequent block (<code>C<sub>1</sub></code>) &rarr; 'E' (ASCII code 69)</li>
                    <li>Second most frequent block (<code>C<sub>2</sub></code>) &rarr; 'T' (ASCII code 84)</li>
                </ul>
            </li>
            <li>
                <strong>Finding <code>n</code> (The Math Trick):</strong>
                <ul class="list-disc list-inside ml-6 mt-2 space-y-2">
                    <li>We start with the RSA formula: <code>C &equiv; M<sup>e</sup> (mod n)</code>.</li>
                    <li>This means that <code>C - M<sup>e</sup></code> is an exact multiple of <code>n</code>.
                        Let's call this value <code>K = |C - M<sup>e</sup>| = k &middot; n</code>.</li>
                    <li>The script calculates these <code>K</code> values for the mapped letters (testing common <code>e</code> values, like 3, 5, 7...).
                        <br><code>K<sub>E</sub> = |C<sub>E</sub> - 69<sup>e</sup>| = k<sub>E</sub> &middot; n</code>
                        <br><code>K<sub>T</sub> = |C<sub>T</sub> - 84<sup>e</sup>| = k<sub>T</sub> &middot; n</code>
                    </li>
                    <li><code>n</code> is a common divisor of all these <code>K</code> values. By calculating the
                    <strong>Greatest Common Divisor (GCD)</strong> of <code>K<sub>E</sub></code>, <code>K<sub>T</sub></code>, ...
                    we get <code>n</code> (or a small multiple of it, e.g., <code>2n</code>).</li>
                </ul>
            </li>
            <li>
                <strong>Factoring <code>n</code>:</strong> Once <code>n</code> is found (e.g., 323),
                the script factors it to find the primes <code>p</code> and <code>q</code> (e.g., 17 and 19).
            </li>
            <li>
                <strong>Calculating the Private Key <code>d</code>:</strong>
                <ul class="list-disc list-inside ml-6 mt-2">
                    <li>Calculate <code>&phi;(n) = (p-1)(q-1)</code>.</li>
                    <li>Calculate <code>d</code> by finding the modular inverse of <code>e</code>:
                        <br><code>d &equiv; e<sup>-1</sup> (mod &phi;(n))</code>.</li>
                </ul>
            </li>
            <li>
                <strong>Validation:</strong> The script uses the newly found private key <code>(d, n)</code>
                to decrypt <code>C<sub>1</sub></code>. If the result is 'E', the attack was successful, and
                the key is used to decrypt the entire message.
            </li>
        </ol>
    </div>
    
<script src="script.js"></script>
</body>
</html>
